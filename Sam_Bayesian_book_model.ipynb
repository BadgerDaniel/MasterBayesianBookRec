{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytensor\n",
    "#print(pytensor.config.cxx)\n",
    "\n",
    "#set up g++ and openBLAS\n",
    "\n",
    "#import pytensor\n",
    "#print(dir(pytensor.config))\n",
    "\n",
    "#import pytensor\n",
    "#pytensor.config.blas__ldflags = '-LC:\\\\OpenBLAS\\\\lib -lopenblas'\n",
    "#print(pytensor.config.blas__ldflags)\n",
    "\n",
    "#import pytensor\n",
    "#print(\"BLAS flags:\", pytensor.config.blas__ldflags)\n",
    "# print(\"Computation Mode:\", pytensor.config.mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE ADVI (only 1000 rows for testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, precision_score, recall_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_excel(\"book_ratings.xlsx\")\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[['User-ID', 'ISBN', 'Book-Rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Downsample to 1000 random rows for testing**\n",
    "# df = df.sample(n=1000, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode User-ID and ISBN as categorical for indexing\n",
    "df['User-Index'] = df['User-ID'].astype(\"category\").cat.codes\n",
    "df['Book-Index'] = df['ISBN'].astype(\"category\").cat.codes\n",
    "\n",
    "# **Remap indices to contiguous range** (Fixes the IndexError)\n",
    "df['User-Index'] = df['User-Index'].astype(\"category\").cat.codes\n",
    "df['Book-Index'] = df['Book-Index'].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rating counts per user and book\n",
    "user_rating_counts = df.groupby('User-Index')['Book-Rating'].count()\n",
    "book_rating_counts = df.groupby('Book-Index')['Book-Rating'].count()\n",
    "\n",
    "# Avoid division by zero\n",
    "user_rating_counts[user_rating_counts == 0] = 1\n",
    "book_rating_counts[book_rating_counts == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays for modeling\n",
    "train_user_ids = train_df['User-Index'].values\n",
    "test_user_ids = test_df['User-Index'].values\n",
    "train_book_ids = train_df['Book-Index'].values\n",
    "test_book_ids = test_df['Book-Index'].values\n",
    "train_ratings = train_df['Book-Rating'].values # Using raw ratings for Poisson\n",
    "test_ratings = test_df['Book-Rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 92106\n",
      "Number of unique books: 270149\n"
     ]
    }
   ],
   "source": [
    "# Get updated number of unique users and books\n",
    "num_users = df['User-Index'].nunique()\n",
    "num_books = df['Book-Index'].nunique()\n",
    "\n",
    "print(\"Number of unique users:\", num_users)\n",
    "print(\"Number of unique books:\", num_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set latent dimension \n",
    "latent_dim = 5\n",
    "\n",
    "# Bayesian Probabilistic Matrix Factorization Model with Gamma-Poisson\n",
    "with pm.Model() as model:\n",
    "    # Prior for global mean rating\n",
    "    mu = pm.Gamma(\"mu\", alpha=2, beta=0.5)\n",
    "    \n",
    "    # User and book bias priors\n",
    "    user_bias = pm.Normal(\"user_bias\", mu=0, sigma=1 / np.sqrt(user_rating_counts + 1), shape=num_users)\n",
    "    book_bias = pm.Normal(\"book_bias\", mu=0, sigma=1 / np.sqrt(book_rating_counts + 1), shape=num_books)\n",
    "\n",
    "    # Hierarchical priors for latent factors\n",
    "    sigma_u = pm.HalfCauchy(\"sigma_u\", beta=1)\n",
    "    sigma_b = pm.HalfCauchy(\"sigma_b\", beta=1)\n",
    "    \n",
    "    user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\n",
    "    book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\n",
    "\n",
    "    # Expected rating using Poisson lambda\n",
    "    lambda_rating = pm.math.exp(\n",
    "        mu +\n",
    "        user_bias[train_user_ids] +\n",
    "        book_bias[train_book_ids] +\n",
    "        (user_factors[train_user_ids] * book_factors[train_book_ids]).sum(axis=1)\n",
    "    )\n",
    "\n",
    "    # Poisson likelihood\n",
    "    ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings)\n",
    "    \n",
    "    # Use ADVI for fast variational inference instead of NUTS\n",
    "    print(\"Running Variational Inference (ADVI)...\")\n",
    "    approx = pm.fit(n=50000, method=\"advi\")\n",
    "    trace = approx.sample(draws=2000)\n",
    "\n",
    "# **Extract posterior values manually since PyMC won't sample `ratings_obs`**\n",
    "with model:\n",
    "    print(\"\\nManually Generating Predictions Using Posterior Samples...\")\n",
    "    \n",
    "    # Extract posterior values\n",
    "    mu_post = trace.posterior[\"mu\"].mean().item()\n",
    "    user_bias_post = trace.posterior[\"user_bias\"].mean(dim=(\"chain\", \"draw\")).values\n",
    "    book_bias_post = trace.posterior[\"book_bias\"].mean(dim=(\"chain\", \"draw\")).values\n",
    "    user_factors_post = trace.posterior[\"user_factors\"].mean(dim=(\"chain\", \"draw\")).values\n",
    "    book_factors_post = trace.posterior[\"book_factors\"].mean(dim=(\"chain\", \"draw\")).values\n",
    "\n",
    "    # Compute expected ratings\n",
    "    predicted_ratings = np.exp(\n",
    "        mu_post + \n",
    "        user_bias_post[test_user_ids] + \n",
    "        book_bias_post[test_book_ids] +\n",
    "        (user_factors_post[test_user_ids] * book_factors_post[test_book_ids]).sum(axis=1)\n",
    "    )\n",
    "\n",
    "    print(\"\\nExample of Predicted Ratings (posterior predictive mean):\")\n",
    "    print(predicted_ratings[:5])\n",
    "    \n",
    "# Evaluation Metrics\n",
    "mae = mean_absolute_error(test_ratings, predicted_ratings)\n",
    "rmse = np.sqrt(mean_squared_error(test_ratings, predicted_ratings))\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Variational Inference (ADVI)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9812c67f8d44b9b9a17485441017a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished [100%]: Average Loss = 2.2016e+06\n",
      "c:\\Users\\samue\\OneDrive\\Documents\\Bayesian_Machine_Learning\\BookRec\\MasterBayesianBookRec\\MasterBayesianVenv\\lib\\site-packages\\pytensor\\link\\utils.py:521: UserWarning: <class 'numpy.core._exceptions._ArrayMemoryError'> error does not allow us to add an extra error message\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 32.4 GiB for an array with shape (2000, 2173533) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Variational Inference (ADVI)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m     approx \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39mfit(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madvi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     trace \u001b[38;5;241m=\u001b[39m \u001b[43mapprox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdraws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# **Extract posterior values manually since PyMC won't sample `ratings_obs`**\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model:\n",
      "File \u001b[1;32mc:\\Users\\samue\\OneDrive\\Documents\\Bayesian_Machine_Learning\\BookRec\\MasterBayesianBookRec\\MasterBayesianVenv\\lib\\site-packages\\pymc\\variational\\opvi.py:1583\u001b[0m, in \u001b[0;36mApproximation.sample\u001b[1;34m(self, draws, random_seed, return_inferencedata, **kwargs)\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random_seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1582\u001b[0m     (random_seed,) \u001b[38;5;241m=\u001b[39m _get_seeds_per_chain(random_seed, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 1583\u001b[0m samples: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_dict_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdraws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1584\u001b[0m points \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1585\u001b[0m     {name: np\u001b[38;5;241m.\u001b[39masarray(records[i]) \u001b[38;5;28;01mfor\u001b[39;00m name, records \u001b[38;5;129;01min\u001b[39;00m samples\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   1586\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(draws)\n\u001b[0;32m   1587\u001b[0m )\n\u001b[0;32m   1589\u001b[0m trace \u001b[38;5;241m=\u001b[39m NDArray(\n\u001b[0;32m   1590\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   1591\u001b[0m     test_point\u001b[38;5;241m=\u001b[39m{name: records[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m name, records \u001b[38;5;129;01min\u001b[39;00m samples\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m   1592\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\samue\\OneDrive\\Documents\\Bayesian_Machine_Learning\\BookRec\\MasterBayesianBookRec\\MasterBayesianVenv\\lib\\site-packages\\pymc\\variational\\opvi.py:1553\u001b[0m, in \u001b[0;36mApproximation.sample_dict_fn.<locals>.inner\u001b[1;34m(draws, random_seed)\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random_seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1552\u001b[0m     reseed_rngs(rng_nodes, random_seed)\n\u001b[1;32m-> 1553\u001b[0m _samples \u001b[38;5;241m=\u001b[39m \u001b[43msample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdraws\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(names, _samples))\n",
      "File \u001b[1;32mc:\\Users\\samue\\OneDrive\\Documents\\Bayesian_Machine_Learning\\BookRec\\MasterBayesianBookRec\\MasterBayesianVenv\\lib\\site-packages\\pytensor\\compile\\function\\types.py:1002\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, output_subset, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthunks\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1001\u001b[0m         thunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvm\u001b[38;5;241m.\u001b[39mthunks[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvm\u001b[38;5;241m.\u001b[39mposition_of_error]\n\u001b[1;32m-> 1002\u001b[0m     \u001b[43mraise_with_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_of_error\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;66;03m# old-style linkers raise their own exceptions\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\samue\\OneDrive\\Documents\\Bayesian_Machine_Learning\\BookRec\\MasterBayesianBookRec\\MasterBayesianVenv\\lib\\site-packages\\pytensor\\link\\utils.py:526\u001b[0m, in \u001b[0;36mraise_with_op\u001b[1;34m(fgraph, node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    521\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m error does not allow us to add an extra error message\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    523\u001b[0m     )\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;66;03m# Some exception need extra parameter in inputs. So forget the\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;66;03m# extra long error message in that case.\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_trace)\n",
      "File \u001b[1;32mc:\\Users\\samue\\OneDrive\\Documents\\Bayesian_Machine_Learning\\BookRec\\MasterBayesianBookRec\\MasterBayesianVenv\\lib\\site-packages\\pytensor\\compile\\function\\types.py:992\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, output_subset, *args, **kwargs)\u001b[0m\n\u001b[0;32m    990\u001b[0m     t0_fn \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 992\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m output_subset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m vm(output_subset\u001b[38;5;241m=\u001b[39moutput_subset)\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_defaults()\n",
      "File \u001b[1;32mc:\\Users\\samue\\OneDrive\\Documents\\Bayesian_Machine_Learning\\BookRec\\MasterBayesianBookRec\\MasterBayesianVenv\\lib\\site-packages\\pytensor\\graph\\op.py:531\u001b[0m, in \u001b[0;36mOp.make_py_thunk.<locals>.rval\u001b[1;34m(p, i, o, n, cm)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;129m@is_thunk_type\u001b[39m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrval\u001b[39m(\n\u001b[0;32m    525\u001b[0m     p\u001b[38;5;241m=\u001b[39mp,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    529\u001b[0m     cm\u001b[38;5;241m=\u001b[39mnode_compute_map,\n\u001b[0;32m    530\u001b[0m ):\n\u001b[1;32m--> 531\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m cm:\n\u001b[0;32m    533\u001b[0m         entry[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\samue\\OneDrive\\Documents\\Bayesian_Machine_Learning\\BookRec\\MasterBayesianBookRec\\MasterBayesianVenv\\lib\\site-packages\\pytensor\\tensor\\random\\op.py:402\u001b[0m, in \u001b[0;36mRandomVariable.perform\u001b[1;34m(self, node, inputs, outputs)\u001b[0m\n\u001b[0;32m    398\u001b[0m     rng \u001b[38;5;241m=\u001b[39m deepcopy(rng)\n\u001b[0;32m    400\u001b[0m outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m rng\n\u001b[0;32m    401\u001b[0m outputs[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m--> 402\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    403\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m    404\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\samue\\OneDrive\\Documents\\Bayesian_Machine_Learning\\BookRec\\MasterBayesianBookRec\\MasterBayesianVenv\\lib\\site-packages\\pytensor\\tensor\\random\\op.py:175\u001b[0m, in \u001b[0;36mRandomVariable.rng_fn\u001b[1;34m(self, rng, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrng_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, rng, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    174\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sample a numeric random variate.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(rng, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mnumpy\\\\random\\\\_generator.pyx:1220\u001b[0m, in \u001b[0;36mnumpy.random._generator.Generator.normal\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:600\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:508\u001b[0m, in \u001b[0;36mnumpy.random._common.cont_broadcast_2\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 32.4 GiB for an array with shape (2000, 2173533) and data type float64"
     ]
    }
   ],
   "source": [
    "# Set latent dimension \n",
    "latent_dim = 5\n",
    "\n",
    "# Bayesian Probabilistic Matrix Factorization Model with Gamma-Poisson\n",
    "with pm.Model() as model:\n",
    "    # Prior for global mean rating\n",
    "    mu = pm.Gamma(\"mu\", alpha=2, beta=0.5)\n",
    "    \n",
    "    # User and book bias priors\n",
    "    user_bias = pm.Normal(\"user_bias\", mu=0, sigma=1 / np.sqrt(user_rating_counts + 1), shape=num_users)\n",
    "    book_bias = pm.Normal(\"book_bias\", mu=0, sigma=1 / np.sqrt(book_rating_counts + 1), shape=num_books)\n",
    "\n",
    "    # Hierarchical priors for latent factors\n",
    "    sigma_u = pm.HalfCauchy(\"sigma_u\", beta=1)\n",
    "    sigma_b = pm.HalfCauchy(\"sigma_b\", beta=1)\n",
    "    \n",
    "    user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\n",
    "    book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\n",
    "\n",
    "    # Expected rating using Poisson lambda\n",
    "    lambda_rating = pm.math.exp(\n",
    "        mu +\n",
    "        user_bias[train_user_ids] +\n",
    "        book_bias[train_book_ids] +\n",
    "        (user_factors[train_user_ids] * book_factors[train_book_ids]).sum(axis=1)\n",
    "    )\n",
    "\n",
    "    # Poisson likelihood\n",
    "    ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings)\n",
    "    \n",
    "    # Use ADVI for fast variational inference instead of NUTS\n",
    "    print(\"Running Variational Inference (ADVI)...\")\n",
    "    approx = pm.fit(n=50000, method=\"advi\")\n",
    "    trace = approx.sample(draws=2000)\n",
    "\n",
    "# **Extract posterior values manually since PyMC won't sample `ratings_obs`**\n",
    "with model:\n",
    "    print(\"\\nManually Generating Predictions Using Posterior Samples...\")\n",
    "    \n",
    "    # Extract posterior values\n",
    "    mu_post = trace.posterior[\"mu\"].mean().item()\n",
    "    user_bias_post = trace.posterior[\"user_bias\"].mean(dim=(\"chain\", \"draw\")).values\n",
    "    book_bias_post = trace.posterior[\"book_bias\"].mean(dim=(\"chain\", \"draw\")).values\n",
    "    user_factors_post = trace.posterior[\"user_factors\"].mean(dim=(\"chain\", \"draw\")).values\n",
    "    book_factors_post = trace.posterior[\"book_factors\"].mean(dim=(\"chain\", \"draw\")).values\n",
    "\n",
    "    # Compute expected ratings\n",
    "    predicted_ratings = np.exp(\n",
    "        mu_post + \n",
    "        user_bias_post[test_user_ids] + \n",
    "        book_bias_post[test_book_ids] +\n",
    "        (user_factors_post[test_user_ids] * book_factors_post[test_book_ids]).sum(axis=1)\n",
    "    )\n",
    "\n",
    "    print(\"\\nExample of Predicted Ratings (posterior predictive mean):\")\n",
    "    print(predicted_ratings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "mae = mean_absolute_error(test_ratings, predicted_ratings)\n",
    "rmse = np.sqrt(mean_squared_error(test_ratings, predicted_ratings))\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of Precision, Recall, MAE, and RMSE\n",
    "\n",
    "def evaluate_predictions(true_ratings, predicted_ratings, threshold=7):\n",
    "    mae = mean_absolute_error(true_ratings, predicted_ratings)\n",
    "    rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\n",
    "    \n",
    "    # Convert to binary relevance (1 if rating >= threshold, else 0)\n",
    "    true_binary = (true_ratings >= threshold).astype(int)\n",
    "    predicted_binary = (predicted_ratings >= threshold).astype(int)\n",
    "    \n",
    "    precision = precision_score(true_binary, predicted_binary, average='micro')\n",
    "    recall = recall_score(true_binary, predicted_binary, average='micro')\n",
    "    \n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# Running evaluation\n",
    "predicted_train_ratings = np.exp(\n",
    "    trace.posterior[\"mu\"].mean().item() +\n",
    "    trace.posterior[\"user_bias\"].mean(dim=(\"chain\", \"draw\")).values[train_user_ids] +\n",
    "    trace.posterior[\"book_bias\"].mean(dim=(\"chain\", \"draw\")).values[train_book_ids]\n",
    ")\n",
    "predicted_test_ratings = np.exp(\n",
    "    trace.posterior[\"mu\"].mean().item() +\n",
    "    trace.posterior[\"user_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids] +\n",
    "    trace.posterior[\"book_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids]\n",
    ")\n",
    "\n",
    "evaluate_predictions(train_ratings, predicted_train_ratings)\n",
    "evaluate_predictions(test_ratings, predicted_test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Bayes General Multi-Step Lookahead Recommendation ---- #\n",
    "\n",
    "def bayes_general_recommendation(user_index, book_indices, trace, top_k=5, exploration_factor=0.5, regret_threshold=0.8, max_regret=2.0):\n",
    "    \"\"\"\n",
    "    Multi-step lookahead Bayesian regret minimization for recommending 5 books.\n",
    "    \"\"\"\n",
    "    mu_samples = trace.posterior[\"mu\"].values\n",
    "    user_bias_samples = trace.posterior[\"user_bias\"].values[:, :, user_index]\n",
    "    book_bias_samples = trace.posterior[\"book_bias\"].values[:, :, book_indices]\n",
    "    user_factors_samples = trace.posterior[\"user_factors\"].values[:, :, user_index, :]\n",
    "    book_factors_samples = trace.posterior[\"book_factors\"].values[:, :, book_indices, :]\n",
    "\n",
    "    num_samples = mu_samples.shape[1]  # Number of posterior samples\n",
    "    \n",
    "    # Compute expected rewards using posterior sampling\n",
    "    expected_rewards = np.mean(\n",
    "        np.exp(mu_samples[:, :, None] + user_bias_samples[:, :, None] + book_bias_samples +\n",
    "               np.sum(user_factors_samples[:, :, None, :] * book_factors_samples, axis=-1)), axis=1\n",
    "    )\n",
    "\n",
    "    # Compute variance (uncertainty measure)\n",
    "    rating_uncertainty = np.var(\n",
    "        np.exp(mu_samples[:, :, None] + user_bias_samples[:, :, None] + book_bias_samples +\n",
    "               np.sum(user_factors_samples[:, :, None, :] * book_factors_samples, axis=-1)), axis=1\n",
    "    )\n",
    "    \n",
    "    # Compute Bayesian regret\n",
    "    best_expected_reward = np.max(expected_rewards, axis=1)\n",
    "    regrets = best_expected_reward[:, None] - expected_rewards\n",
    "\n",
    "    # Cap regret to prevent extreme exploration\n",
    "    regrets = np.clip(regrets, 0, max_regret)\n",
    "\n",
    "    # Apply regret threshold\n",
    "    should_explore = regrets > regret_threshold\n",
    "\n",
    "    # Compute future learning potential\n",
    "    expected_future_gain = exploration_factor * rating_uncertainty\n",
    "\n",
    "    # Compute exploration-adjusted score\n",
    "    exploration_score = expected_rewards + expected_future_gain\n",
    "\n",
    "    # Rank books\n",
    "    ranked_books = np.argsort(-exploration_score, axis=1)  # Sort in descending order\n",
    "\n",
    "    # Select top-k books for recommendation\n",
    "    selected_books = [book_indices[i] for i in ranked_books[0, :top_k]]\n",
    "\n",
    "    return selected_books\n",
    "\n",
    "# Example usage: Recommend 5 books for a user\n",
    "user_id_example = 42  # Replace with an actual user ID\n",
    "book_pool = np.arange(num_books)  # Assuming all books are available\n",
    "\n",
    "recommended_books = bayes_general_recommendation(user_id_example, book_pool, trace, top_k=5)\n",
    "print(\"\\nTop-5 Recommended Books for User\", user_id_example, \":\", recommended_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of Precision, Recall, MAE, and RMSE in the top 5 recommendations\n",
    "\n",
    "def evaluate_recommendations(user_ids, book_pool, trace, top_k=5):\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_mae = 0\n",
    "    total_rmse = 0\n",
    "    user_count = len(user_ids)\n",
    "    \n",
    "    for user in user_ids:\n",
    "        actual_books = set(test_df[test_df['User-Index'] == user]['Book-Index'].values)\n",
    "        actual_ratings = test_df[test_df['User-Index'] == user]['Book-Rating'].values\n",
    "        recommended_books = set(bayes_general_recommendation(user, book_pool, trace, top_k))\n",
    "        \n",
    "        if len(actual_books) > 0:\n",
    "            precision = len(recommended_books & actual_books) / top_k\n",
    "            recall = len(recommended_books & actual_books) / len(actual_books)\n",
    "            predicted_ratings = np.array([trace.posterior[\"mu\"].mean().item() + trace.posterior[\"user_bias\"].mean(dim=(\"chain\", \"draw\")).values[user] + trace.posterior[\"book_bias\"].mean(dim=(\"chain\", \"draw\")).values[book] for book in actual_books])\n",
    "            \n",
    "            mae = mean_absolute_error(actual_ratings, predicted_ratings)\n",
    "            rmse = np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "        else:\n",
    "            precision = 0\n",
    "            recall = 0\n",
    "            mae = 0\n",
    "            rmse = 0\n",
    "        \n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_mae += mae\n",
    "        total_rmse += rmse\n",
    "    \n",
    "    avg_precision = total_precision / user_count\n",
    "    avg_recall = total_recall / user_count\n",
    "    avg_mae = total_mae / user_count\n",
    "    avg_rmse = total_rmse / user_count\n",
    "    \n",
    "    print(f\"Average Precision@{top_k}: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall@{top_k}: {avg_recall:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {avg_mae:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {avg_rmse:.4f}\")\n",
    "\n",
    "# Running evaluation\n",
    "book_pool = np.arange(num_books)\n",
    "evaluate_recommendations(test_user_ids, book_pool, trace, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterBayesianVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
