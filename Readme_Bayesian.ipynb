{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Book Recommendation System Using Bayesian Machine Learning**\n",
        "**University of Chicago MS in Applied Data Science**\n",
        "\n",
        "**Course:** Bayesian Machine Learning with Generative AI Applications\n",
        "\n",
        "**Date:** 03/13/2025\n",
        "\n",
        "**Contributors:**\n",
        "- Sam Fisher  \n",
        "- Daniel Sa  \n",
        "- Jazil Karim  \n",
        "- Amy (Hyunji) Kim  "
      ],
      "metadata": {
        "id": "CcQaRTzpZipq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1 Introduction**  [ Jazil ]\n",
        "\n",
        "Traditional recommendation systems often rely on the most common items or collaborative filtering techniques, which group users and items based on similarily measures. While these methods can be effective, they struggle in scenarios where data is sparse or user preferences evolve over time, leading to suboptimal recommendations.\n",
        "\n",
        "Bayesian collaborative filtering enhances recommendation quality by incorporating probability-based predictions. Rather than solely relying on similarity-based heuristics, this approach estimates the likelihood of a user enjoying an item based on observed ratings and latent factors. A key component of this model is the likelihood function, which captures the probability distribution of observed ratings based on user-item interactions. By leveraging Bayesian principles, the model introduces latent variables to capture hidden factors influencing user preferences and dynamically updates its predictions as new data becomes available, offering a more robust and flexible recommendation framework.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wmW4o_4weCV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2 Project Summary**\n",
        "\n",
        "### 2.1 Bayesian Recommendation System for Book Ratings\n",
        "\n",
        "In this project, we develop a **Bayesian Recommendation System** to predict how users will rate books they have not yet seen. Unlike traditional matrix factorization techniques such as **Singular Value Decomposition (SVD)**, our approach leverages a **Bayesian Network (Bayes Net)** to incorporate user biases, item biases, and latent factors while also quantifying uncertainty in predictions.  \n",
        "\n",
        "### 2.2 Model Overview  \n",
        "\n",
        "Our model represents each user with a **latent preference vector (αᵤ)** and each book with a **latent attribute vector (βᵢ)**. Both are drawn from prior distributions that help  regularize the model and prevent overfitting. The predicted rating follows a **probabilistic distribution** which incorporates:  \n",
        "\n",
        "- A **global rating mean**: Captures overall rating tendencies\n",
        "- A **user bias term**: Reflects individual rating tendencies (How lenient or harsh a user typically rates books)\n",
        "- A **book bias term**: Represents the general quality of a book across all users\n",
        "- The **dot product of preference and attribute vectors**: Represents user-item interactions\n",
        "\n",
        "### 2.3 Inference & Estimation  \n",
        "\n",
        "To estimate model parameters, we employ **Markov Chain Monte Carlo (MCMC)** for robust posterior estimation. Additionally, we implement **Automatic Differentiation Variational Inference (ADVI)** as our final inference method to enhance computational efficiency and ensure adaptability to different datasets and user behaviors. All Bayesian parameters are learned directly from the data.  \n",
        "\n",
        "### 2.4 Dataset [ Daniel ]\n",
        "\n",
        "The project utilizes the **Book Recommendation Dataset** sourced from Kaggle, which contains real-world book ratings and user interactions: [Dataset Link](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset)\n",
        "\n",
        "**Origin & Content**\n",
        "\n",
        "This dataset is derived from the **Book-Crossing dataset**, originally compiled by Cai-Nicolas Ziegler in 2004. Book-Crossing was an online community where users freely shared their book ratings and reviews.\n",
        "\n",
        "The dataset provides a rich source of information for building a recommendation system including:\n",
        "\n",
        "- User Information: Anonymized data about users, such as age and location.\n",
        "- Book Metadata: Details about books, including titles, authors, publication years, and publishers.\n",
        "- User Ratings: Explicit ratings given by users on a scale from 0 to 10. A rating of 0 indicates an implicit interaction (e.g., the user might have shown interest in the book without explicitly rating it).\n",
        "\n",
        "With over **500,000 ratings from more than 100,000 users**, this dataset offers a substantial and diverse collection of user preferences and book characteristics. This allows for robust training of a recommendation model.\n",
        "\n",
        "**Processed Dataset**\n",
        "\n",
        "The project uses a structured version of the dataset, which includes the following fields:\n",
        "\n",
        "- User-ID: A unique identifier for each user.\n",
        "- ISBN: The International Standard Book Number, a unique commercial book identifier.\n",
        "- Book-Rating: The rating given by the user to the book.\n",
        "\n",
        "Here's a glimpse of the processed data:\n",
        "\n",
        "| User-ID | ISBN       | Book-Rating |\n",
        "|---------|------------|-------------|\n",
        "| 276725  | 034545104X | 0           |\n",
        "| 276726  | 0155061224 | 5           |\n",
        "| 276727  | 0446605239 | 0           |\n",
        "| 276729  | 052165615X | 3           |\n",
        "| 276729  | 0521795028 | 6           |\n",
        "\n",
        "This processed data will be pivotal in training our Bayesian model to accurately predict user preferences and provide personalized book recommendations.\n",
        "\n",
        "### 2.5 Model Evaluation  [ AMY ]\n",
        "\n",
        "We assess the model’s performance using various metrics, including:  \n",
        "\n",
        "- **Precision** - Measures the proportion of relevant recommendations among the total recommendations.\n",
        "- **Recall** - Assesses the proportion of relevant items that were successfully recommended.\n",
        "- **Root Mean Squared Error (RMSE)** - Evaluates the accuracy of rating predictions by measuring the deviation from actual ratings.\n",
        "\n",
        "By leveraging Bayesian techniques, our model offers improved recommendation accuracy while incorporating uncertainty estimation, leading to more reliable and interpretable predictions.\n",
        "\n",
        "### 2.6 Advantages of Bayesian Collaborative Filtering  [ SAM ]\n",
        "\n",
        "Bayesian collaborative filtering offers several advantages over traditional filtering approaches:\n",
        "\n",
        "1. **Handling Uncertainty**  \n",
        "   Bayesian methods excel at managing uncertainty by incorporating prior knowledge. They dynamically update predictions as new data becomes available, making them particularly useful in environments where user preferences change frequently.\n",
        "\n",
        "2. **Addressing the Cold Start Problem**  \n",
        "   Bayesian models can better handle the cold start problem by utilizing prior distributions based on historical data or demographic information. This allows the system to make educated and improved recommendations for new users and items.\n",
        "\n",
        "3. **Integration of Prior Knowledge**  \n",
        "   These methods allow for the incorporation of domain-specific prior knowledge, making them especially useful when data is limited. This leads to more informed and accurate recommendations.\n",
        "\n",
        "4. **Flexibility and Adaptability**  \n",
        "   Bayesian models are adaptable to various data types and recommendation scenarios. They can seamlessly integrate multiple data sources, including user behavior, item attributes, and contextual factors, leading to more relevant recommendation.\n",
        "\n",
        "5. **Probabilistic Framework**  \n",
        "   By providing a probabilistic framework, Bayesian methods quantify uncertainty in recommendations, leading to more interpretable and reliable suggestions.\n",
        "\n",
        "6. **Scalability**  \n",
        "   Bayesian models can scale effectively with large datasets, making them more suitable for real-world applications compared to traditional collaborative filtering methods, particularly when dealing with large datasets.\n",
        "\n",
        "7. **Enhanced Personalization**  \n",
        "   By capturing user-specific latent factors, Bayesian approaches offer highly tailored recommendations that adapt to individual preferences.\n",
        "\n",
        "By leveraging Bayesian collaborative filtering, recommendation systems can achieve greater adaptability, reliability, and personalization, ultimately improving user experience across various applications.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Hji_nSPleV2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3 Theory: Collaborative Filtering Model**  [ Daniel ]\n",
        "\n",
        "This section describes a collaborative filtering model designed to predict user ratings for books. The model leverages past user-book interactions to suggest books to users.\n",
        "\n",
        "### 3.1 Equation for Predicting Ratings\n",
        "\n",
        "The predicted rating $r_{u,i}$ (the rating user $u$ gives to item $i$) is modeled as a gamma poisson:\n",
        "\n",
        "$$\n",
        "r_{u, i} \\sim \\mathcal{GammaPoisson}\\left(\\mu+b_{u}+b_{i}+\\alpha_{u}^{T} \\beta_{i}, \\sigma^{2}\\right)\n",
        "$$\n",
        "\n",
        "Where the mean of the distribution is the sum of the following components:\n",
        "\n",
        "*   $\\mu$: The global average rating across all users and books. This represents the overall popularity or quality of books in general.\n",
        "\n",
        "*   $b_{u}$: The user bias. This term captures the tendency of a user to give consistently higher or lower ratings than average.\n",
        "\n",
        "*   $b_{i}$: The item (book) bias. This term captures the tendency of a book to receive consistently higher or lower ratings than average.\n",
        "\n",
        "*   $\\alpha_{u}^{T} \\beta_{i}$: The dot product of the user preference vector $\\alpha_{u}$ and the book feature vector $\\beta_{i}$. This term captures the interaction between specific user preferences and specific book characteristics.  $\\alpha_u$ represents the latent (hidden) preferences of user $u$, while $\\beta_i$ represents the latent features of book $i$. The dot product captures how well the user's preferences align with the book's features.\n",
        "\n",
        "The variance of this normal distribution is $\\sigma^{2}$, representing the overall noise or variability in the ratings.\n",
        "\n",
        "### 3.2 Priors on Model Parameters  \n",
        "\n",
        "To regularize the model and prevent overfitting, prior distributions are placed on the model parameters:\n",
        "\n",
        "*   **Global Rating Prior:**\n",
        "\n",
        "    *   $\\mu \\sim \\mathcal{N}(0,5)$\n",
        "    *   The global rating $\\mu$ follows a normal distribution with a mean of 0 and a variance of 5.\n",
        "    * This prior allows the model to learn an appropriate global rating, while regularizing it towards zero.\n",
        "\n",
        "*   **User Biases:**\n",
        "\n",
        "    *   $b_{u} \\sim \\mathcal{N}(0,1)$ for all users\n",
        "    *   Each user bias $b_{u}$ follows a normal distribution with a mean of 0 and a variance of 1.\n",
        "    * This prior encourages user biases to be small, preventing individual users from unduly influencing the overall rating predictions.\n",
        "\n",
        "*   **Book Biases:**\n",
        "\n",
        "    *   $b_{i} \\sim \\mathcal{N}(0,1)$ for all books\n",
        "    *   Each book bias $b_{i}$ follows a normal distribution with a mean of 0 and a variance of 1.\n",
        "    * This prior encourages book biases to be small, preventing individual books from unduly influencing the overall rating predictions. Some books might get consistently good or bad ratings, and this allows the model to capture that.\n",
        "\n",
        "*   **Latent Book Features:**\n",
        "\n",
        "    *   $\\beta_{i} \\sim \\mathcal{N}(0, I \\sigma_{\\beta}^{2})$\n",
        "    *   Each book feature vector $\\beta_{i}$ follows a normal distribution centered at zero with identity covariance matrix scaled by $\\sigma_{\\beta}^{2}$.\n",
        "\n",
        "*   **Latent User Preferences:**\n",
        "    *   $\\alpha_{u} \\sim \\mathcal{N}(0, I \\sigma_{\\alpha}^{2})$\n",
        "    *   Each user preference vector $\\alpha_{u}$ follows a normal distribution centered at zero with identity covariance matrix scaled by $\\sigma_{\\alpha}^{2}$.\n",
        "\n",
        "### 3.3 Summary\n",
        "This model predicts user ratings by combining a global average rating with user-specific and book-specific biases, as well as a term that captures the interaction between user preferences and book features. Prior distributions are used to regularize the model parameters for better generalization and improved recommendation accuracy by preventing overfitting."
      ],
      "metadata": {
        "id": "gK2TFWJ4FnIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **4 Bayesian Recommendation Modeling**  [ JAZIL ]\n",
        "\n",
        "###  4.1 Data Preparation for Probabilistic Modeling\n",
        "\n",
        "The dataset consists of user-book interactions, represented by ratings. We extract key components:  \n",
        "\n",
        "- **Number of unique users**: `num_users`  \n",
        "- **Number of unique books**: `num_books`  \n",
        "- **User IDs**: `user_ids`  \n",
        "- **Book IDs**: `book_ids`  \n",
        "- **Ratings given by users**: `ratings`  \n",
        "\n",
        "A hyperparameter, **latent dimension (latent_dim)**, is set to **2** to control the complexity of latent user and book factors.  \n",
        "\n",
        "To adjust priors based on sparsity:  \n",
        "\n",
        "- **User rating counts**: Number of books each user has rated  \n",
        "- **Book rating counts**: Number of ratings each book has received  \n",
        "\n",
        "\n",
        "### 4.2 Bayesian Model Initialization\n",
        "\n",
        "The rating prediction is based on:  \n",
        "\n",
        "- **Global mean rating (`mu`)**: Modeled with a Gamma prior (α=2, β=0.5)  \n",
        "- **User bias (`user_bias`)**: Normally distributed with mean **0** and variance **1 / sqrt(user_rating_count)**  \n",
        "- **Book bias (`book_bias`)**: Modeled similarly to user bias but for books  \n",
        "\n",
        "**Hierarchical Priors for Latent Factors:**\n",
        "\n",
        "Bayesian models use **hierarchical priors** to capture uncertainty at multiple levels.  \n",
        "\n",
        "- **Standard deviation priors**:  \n",
        "  - `sigma_u` (user factors) ~ Half-Cauchy(β=1)  \n",
        "  - `sigma_b` (book factors) ~ Half-Cauchy(β=1)  \n",
        "\n",
        "- **Latent factor distributions**:  \n",
        "  - `user_factors` ~ Normal(0, `sigma_u`) → Shape: (num_users, latent_dim)  \n",
        "  - `book_factors` ~ Normal(0, `sigma_b`) → Shape: (num_books, latent_dim)  \n",
        "\n",
        "These factors represent underlying user preferences and book attributes.  \n",
        "\n",
        "\n",
        "### 4.3 Rating Prediction\n",
        "\n",
        "The predicted rating (`lambda_rating`) is computed as:\n",
        "the combination of user biases, book biases, and latent factors.\n",
        "$$\n",
        "\\lambda_{rating} = \\exp(\\mu + user\\_bias[user] + book\\_bias[book] + (user\\_factors[user] \\cdot book\\_factors[book]))\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- `mu` is the global average rating.\n",
        "- `user_bias` and `book_bias` capture user-specific and book-specific biases in rating tendencies.\n",
        "- `user_factors[user] ⋅ book_factors[book]` represents the interaction between user and book latent factors. This captures how much a user's preferences align with a book's characteristics.\n",
        "\n",
        "\n",
        "### 4.4 Likelihood Function (Poisson Distribution)\n",
        "\n",
        "Observed ratings (`ratings_obs`) follow a **Poisson likelihood**:\n",
        "\n",
        "$$\n",
        "ratings\\_obs \\sim \\text{Poisson}(\\lambda_{\\text{rating}})\n",
        "$$\n",
        "\n",
        "This allows the model to learn parameters (user/book biases and latent factors) that best explain observed ratings.\n",
        "\n",
        "\n",
        "### 4.5 Bayesian Inference & Model Training\n",
        "\n",
        "The model parameters are estimated using **Bayesian inference**, which updates prior beliefs based on observed data to compute **posterior distributions**. We use:  \n",
        "\n",
        "- **Markov Chain Monte Carlo (MCMC)** for robust posterior estimation  \n",
        "- **Automatic Differentiation Variational Inference (ADVI)** as an alternative for faster convergence  \n",
        "\n",
        "\n",
        "### 4.6 Model Updates & Learning\n",
        "\n",
        "During training, the model iteratively updates:  \n",
        "\n",
        "1. **Error Calculation**  \n",
        "   - Example: `error = observed_rating - predicted_rating`  \n",
        "\n",
        "2. **Parameter Updates**  \n",
        "   - `user_bias[user] += learning_rate * error`  \n",
        "   - `book_bias[book] += learning_rate * error`  \n",
        "   - `user_factors[user] += learning_rate * error * book_factors[book]`  \n",
        "   - `book_factors[book] += learning_rate * error * user_factors[user]`  \n",
        "\n",
        "This process minimizes the difference between predicted and actual ratings.  "
      ],
      "metadata": {
        "id": "4K92G9xGPf78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## **5 Multi-Step Lookahead & Bayesian Regret Minimization Strategy** [ SAM ]\n",
        "\n",
        "Our recommendation system employs a **Bayesian regret minimization strategy** that uses **multi-step lookahead** to optimize the selection of recommended books. This method balances the trade-off between **exploitation** (recommending books we are confident the user will enjoy) and **exploration** (suggesting books where the system is less certain but could learn valuable information).\n",
        "\n",
        "For each user, we compute:\n",
        "\n",
        "- **Expected reward**: The mean predicted rating for each book, based on posterior samples from our Bayesian model.\n",
        "- **Uncertainty (variance)**: The variability in those predictions, which represents how uncertain the system is about the user’s potential rating of a book.\n",
        "- **Regret**: The difference between the best possible expected reward and the expected reward for each book.\n",
        "\n",
        "By combining these components, the system assigns an **exploration score** to each book:\n",
        "\n",
        "$$\n",
        "\\text{Exploration Score} = \\text{Expected Reward} + (\\text{Exploration Factor} \\times \\text{Uncertainty})\n",
        "$$\n",
        "\n",
        "Books with **higher uncertainty** and **potential learning benefit** are prioritized when their **regret** exceeds a defined threshold. This lookahead mechanism ensures that some recommendations actively reduce uncertainty about user preferences, improving the model's long-term learning.\n",
        "\n",
        "Ultimately, the **multi-step lookahead approach** selects a top-`k` list of books that balances immediate relevance with future learning potential, driving both short-term user satisfaction and long-term system improvement.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "a6OJesCAAwXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6 Results and Evaluaton**  [ AMY ]\n",
        "\n",
        "We assess the model’s performance using various metrics, including:  \n",
        "\n",
        "\n",
        "- **Precision**\n",
        "  We fix a threshold of 7 if rating is good or bad and convert rating to binary. Then we calculate recall on test set which is out of all predicted positive reviews, how many were actually positive. This is coming out as 60%\n",
        "\n",
        "- **Recall**  \n",
        "  Similary, for all actual true positive values, how many did we correctly identified as positive. This is coming out as 61%\n",
        "\n",
        "  - **Root Mean Squared Error (RMSE)**  \n",
        " Once we have our predicted ratings, we can compare them with our test dataset to evaluate and RMSE is coming out to be 1.49"
      ],
      "metadata": {
        "id": "PYu7VLdFekk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7 Takeaways**\n",
        "\n",
        "- Model prediction time is nothing to sneeze at![timerunning.png](data:image/png;base64,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)\n",
        "- Accurate priors are everything in BML\n",
        "- Leave Hyperparameter tuning for last\n",
        "- Sampling can be more computationally intensive than you might think.\n"
      ],
      "metadata": {
        "id": "fOPBGnocRjZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8 Conclusions & Future Improvements**  [ Daniel ]\n",
        "- Introduce variables of book and author metadata to improve predictions.\n",
        "- Introduce variables of user metadata to improve predictions.\n",
        "- Use alternative metrics to improve fine-tuning (Diversity, Coverage, NDCG, MRR)\n",
        "- Utilize superior hardware to train better models (more latent dimensions, more data)"
      ],
      "metadata": {
        "id": "Go37yZ81ekfO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lzntpG1XFBNd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}