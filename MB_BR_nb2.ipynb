{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import will2live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this cell is everything done in the previous notebook, avoiding having to mess with file saving to make things simpler ( and avoid git max size issues) \n",
    "### any edits to the cleaning-- we should do in the previous notebook and later paste them here  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File 'archive.zip' not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_27072\\1884933557.py:36: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  book_df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import Levenshtein\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def unpack_zip_to_parent(zip_path):\n",
    "    \"\"\"Unpacks the given .zip file to the parent directory of the current folder.\"\"\"\n",
    "    \n",
    "    current_dir = os.getcwd()\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "    # Ensure the zip file exists\n",
    "    if not os.path.isfile(zip_path):\n",
    "        print(f\"Error: File '{zip_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(parent_dir)\n",
    "        print(f\"Successfully extracted '{zip_path}' to '{parent_dir}'\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Error: Not a valid ZIP file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "zip_file = \"archive.zip\"\n",
    "unpack_zip_to_parent(zip_file)\n",
    "\n",
    "parent_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "file_path = os.path.join(parent_directory, \"Books.csv\")\n",
    "book_df = pd.read_csv(file_path)\n",
    "file_path = os.path.join(parent_directory, \"Users.csv\")\n",
    "user_df = pd.read_csv(file_path)\n",
    "file_path = os.path.join(parent_directory, \"Ratings.csv\")\n",
    "ratings_df = pd.read_csv(file_path)\n",
    "\n",
    "book_df.head()\n",
    "book_df['Book-Author'].value_counts()\n",
    "\n",
    "## Agatha Christie had over 100 books, but not over 600.  So there will be some cleaning we need to do here. \n",
    "author_counts = book_df['Book-Author'].value_counts()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(author_counts, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Author Book Counts')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('')\n",
    "plt.yscale('log') \n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "duplicates = book_df.groupby('Book-Title')['ISBN'].nunique()\n",
    "multiple_isbns = duplicates[duplicates > 1]\n",
    "\n",
    "# see where books have multiple isbns\n",
    "duped_titles = book_df[book_df['Book-Title'].isin(multiple_isbns.index)].sort_values(by = 'Book-Title')\n",
    "duped_titles[duped_titles['Book-Author']=='Agatha Christie']\n",
    "author_counts = book_df['Book-Author'].value_counts()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(author_counts, bins=50, edgecolor='black', alpha=0.7)\n",
    "\n",
    "book_df = book_df.sort_values(by=['Book-Author', 'Book-Title', 'ISBN'])\n",
    "plt.xlabel('Author Book Counts')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('')\n",
    "plt.yscale('log') \n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "ratings_df['User-ID'].value_counts()\n",
    "\n",
    "### so we'll want to be aware of both the cold-start problem, \n",
    "### and some readers who might either be bots or just anomalies\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "user_id_counts = ratings_df['User-ID'].value_counts()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(user_id_counts, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Ratings per User')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Ratings per User')\n",
    "plt.yscale('log')  # Log scale to better visualize the distribution\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "user_df\n",
    "df = ratings_df.merge(book_df,\n",
    "                  left_on=\"ISBN\",\n",
    "                  right_on=\"ISBN\").merge(\n",
    "                      user_df,left_on='User-ID',right_on='User-ID')\n",
    "df.head(3)\n",
    "\n",
    "df.info()\n",
    "for i in df.columns:\n",
    "    print(i,\"percent missing: \",(len(df[df[i].isna()])/len(df)))\n",
    "    \n",
    "\n",
    "df[df['Book-Author'].isna()]\n",
    "df = df.dropna(subset=['Book-Author'])\n",
    "df.drop(['Image-URL-M','Image-URL-L'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "## This should only be run once, if it works. \n",
    "\n",
    "'''\n",
    "#### Identify Duplicates (Same Title + Author, Different ISBN)\n",
    "isbn_counts = df.groupby(['Book-Title', 'Book-Author'])['ISBN'].nunique()\n",
    "duplicate_books = isbn_counts[isbn_counts > 1].index  # Titles with multiple ISBNs\n",
    "\n",
    "#### Choose a Representative ISBN for Each Title-Author Combo\n",
    "isbn_mapping = {}\n",
    "\n",
    "for title, author in tqdm(duplicate_books, desc=\"Processing Duplicate Books\", unit=\"book\"):\n",
    "    subset = df[(df['Book-Title'] == title) & (df['Book-Author'] == author)]\n",
    "    \n",
    "    # Pick the most frequent ISBN, fallback to earliest ISBN if there's a tie\n",
    "    common_isbn = subset['ISBN'].value_counts().idxmax()\n",
    "    \n",
    "    # Store mapping\n",
    "    isbn_mapping[(title, author)] = common_isbn\n",
    "\n",
    "#### Replace ISBNs in DataFrame\n",
    "df['ISBN_fix'] = df.apply(\n",
    "    lambda row: isbn_mapping.get((row['Book-Title'], row['Book-Author']), row['ISBN']),\n",
    "    axis=1\n",
    ")\n",
    "'''\n",
    "\n",
    "#isbn_df = pd.DataFrame(isbn_mapping.items(), columns=['Title_Author', 'ISBN'])\n",
    "#isbn_df.to_csv(\"isbn_mapping.csv\", index=False)\n",
    "loaded_mapping_df = pd.read_csv(\"isbn_mapping.csv\")\n",
    "loaded_mapping = dict(zip(loaded_mapping_df['Title_Author'], loaded_mapping_df['ISBN']))\n",
    "\n",
    "df['Title_Author'] = df['Book-Title'] + \" || \" + df['Book-Author']  # Match format used in mapping\n",
    "df['ISBN_fix'] = df['Title_Author'].map(loaded_mapping).fillna(df['ISBN'])\n",
    "\n",
    "df.drop(columns=['Title_Author'], inplace=True)\n",
    "loaded_mapping_df = pd.read_csv(\"isbn_mapping.csv\")\n",
    "\n",
    "loaded_mapping = dict(zip(loaded_mapping_df['Title_Author'], loaded_mapping_df['ISBN']))\n",
    "\n",
    "df[df['User-ID']==11676]\n",
    "df['Location'].value_counts()\n",
    "## maybe we can clean this to start, country, or drop.  \n",
    "df[df['Year-Of-Publication']==0].sort_values(by='Book-Rating',ascending=False)\n",
    "### so maybe a API call and go off of the ISBN,  then use something simple like\n",
    "#  levenschtein distance to make sure the book title and author are close before using data for the fixed Year-Of-Publication\n",
    "\n",
    "\n",
    "### we can also use the levenschtein distance or something similar to determine if there's any books that are really close in title and author but have differnt isbns \n",
    "\n",
    "'''\n",
    "\n",
    "# Google Books API Base URL\n",
    "GOOGLE_BOOKS_API_URL = \"https://www.googleapis.com/books/v1/volumes\"\n",
    "\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "parent_folder = os.path.dirname(current_dir)\n",
    "file_path = os.path.join(parent_folder, 'dans_api_key.txt')\n",
    "with open(file_path, 'r') as file:\n",
    "    API_KEY = file.read()\n",
    "'''\n",
    "df_backup = df.copy(deep=True)\n",
    "df_backup.head()\n",
    "'''\n",
    "# Create combined title-author field for checking similarity\n",
    "df[\"Title_Author\"] = df[\"Book-Title\"].fillna('') + \" \" + df[\"Book-Author\"].fillna('')\n",
    "\n",
    "# Ensure ISBN_fix column exists and clean it\n",
    "df[\"ISBN_fix\"] = df[\"ISBN_fix\"].astype(str).str.strip()\n",
    "\n",
    "# Define conditions for when to query the API\n",
    "def needs_imputation(year):\n",
    "    return pd.isna(year) or str(year).strip() in [\"\", \"0\", \"NA\", \"N/A\", \"null\", \"None\"]\n",
    "\n",
    "# Filter ISBNs that require lookup\n",
    "isbns_to_lookup = df[df[\"Year-Of-Publication\"].apply(needs_imputation)][\"ISBN_fix\"].tolist()\n",
    "\n",
    "# Store results\n",
    "isbn_to_publication_year = {}\n",
    "\n",
    "# Levenshtein similarity threshold (0.8 means 80% similar)\n",
    "SIMILARITY_THRESHOLD = 0.8\n",
    "\n",
    "# Loop through ISBNs that need a Year-Of-Publication with TQDM progress bar\n",
    "for isbn in tqdm(isbns_to_lookup, desc=\"Processing ISBNs\", unit=\"ISBN\"):\n",
    "    params = {\"q\": f\"isbn:{isbn}\", \"key\": API_KEY}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(GOOGLE_BOOKS_API_URL, params=params)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract book details if found\n",
    "        if \"items\" in data:\n",
    "            book = data[\"items\"][0][\"volumeInfo\"]\n",
    "            title = book.get(\"title\", \"Unknown\")\n",
    "            authors = \", \".join(book.get(\"authors\", [\"Unknown\"]))\n",
    "            publication_date = book.get(\"publishedDate\", \"Unknown\")\n",
    "\n",
    "            # Construct title-author string for comparison\n",
    "            found_title_author = f\"{title} {authors}\"\n",
    "\n",
    "            # Get the correct title-author from your dataset\n",
    "            correct_title_author = df.loc[df[\"ISBN_fix\"] == isbn, \"Title_Author\"].values[0]\n",
    "\n",
    "            # Compute Levenshtein similarity\n",
    "            similarity_score = Levenshtein.ratio(correct_title_author.lower(), found_title_author.lower())\n",
    "\n",
    "            if similarity_score >= SIMILARITY_THRESHOLD:\n",
    "                final_publication_date = publication_date  # Accept the found Year-Of-Publication\n",
    "            else:\n",
    "                final_publication_date = \"ISBN Mismatch\"\n",
    "\n",
    "            # Store the found year in a dictionary\n",
    "            isbn_to_publication_year[isbn] = final_publication_date\n",
    "\n",
    "        else:\n",
    "            isbn_to_publication_year[isbn] = \"Not Found\"\n",
    "\n",
    "        time.sleep(1)  # Prevent hitting API rate limits\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        isbn_to_publication_year[isbn] = \"Error\"\n",
    "\n",
    "# Apply the found Year-Of-Publication **without modifying any other columns**\n",
    "df[\"Year-Of-Publication\"] = df.apply(\n",
    "    lambda row: isbn_to_publication_year.get(row[\"ISBN_fix\"], row[\"Year-Of-Publication\"]) \n",
    "    if needs_imputation(row[\"Year-Of-Publication\"]) else row[\"Year-Of-Publication\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"✅ Year-Of-Publication values successfully updated!\")\n",
    "\n",
    "'''\n",
    "'''\n",
    "current_dir = os.getcwd()\n",
    "parent_folder = os.path.dirname(current_dir)\n",
    "csv_file_path = os.path.join(parent_folder, 'output.csv')\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "print(f\"DataFrame saved to {csv_file_path}\")\n",
    "\n",
    "print(\"✅ Year-Of-Publication values successfully updated!\")\n",
    "\n",
    "'''\n",
    "'''\n",
    "# Create a full ISBN_fix-to-Year mapping\n",
    "isbn_mapping = df[[\"ISBN_fix\", \"Year-Of-Publication\"]].set_index(\"ISBN_fix\").to_dict()[\"Year-Of-Publication\"]\n",
    "\n",
    "# Save mapping to CSV\n",
    "isbn_mapping_df = pd.DataFrame(isbn_mapping.items(), columns=[\"ISBN_fix\", \"Year-Of-Publication\"])\n",
    "isbn_mapping_df.to_csv(\"isbn_year_mapping.csv\", index=False)\n",
    "\n",
    "print(\"✅ Full ISBN-to-Year mapping saved as 'isbn_year_mapping.csv'\")\n",
    "'''\n",
    "isbn_mapping_df = pd.read_csv(\"isbn_year_mapping.csv\")\n",
    "isbn_mapping = dict(zip(isbn_mapping_df[\"ISBN_fix\"], isbn_mapping_df[\"Year-Of-Publication\"]))\n",
    "\n",
    "# Function to determine if a value needs imputation\n",
    "def needs_imputation(year):\n",
    "    return pd.isna(year) or str(year).strip() in [\"\", \"0\", \"NA\", \"N/A\", \"null\", \"None\"]\n",
    "\n",
    "# Apply mapping only to missing values\n",
    "df[\"Year-Of-Publication\"] = df.apply(\n",
    "    lambda row: isbn_mapping.get(row[\"ISBN_fix\"], row[\"Year-Of-Publication\"]) \n",
    "    if needs_imputation(row[\"Year-Of-Publication\"]) else row[\"Year-Of-Publication\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"✅ Missing Year-Of-Publication values successfully imputed using saved mapping!\")\n",
    "df\n",
    "df\n",
    "isbn_mapping_df = pd.read_csv(\"isbn_year_mapping.csv\")\n",
    "isbn_mapping = dict(zip(isbn_mapping_df[\"ISBN_fix\"], isbn_mapping_df[\"Year-Of-Publication\"]))\n",
    "\n",
    "# Function to determine if a value needs imputation\n",
    "def needs_imputation(year):\n",
    "    return pd.isna(year) or str(year).strip() in [\"\", \"0\", \"NA\", \"N/A\", \"null\", \"None\"]\n",
    "\n",
    "# Apply mapping only to missing values\n",
    "df[\"Year-Of-Publication\"] = df.apply(\n",
    "    lambda row: isbn_mapping.get(row[\"ISBN_fix\"], row[\"Year-Of-Publication\"]) \n",
    "    if needs_imputation(row[\"Year-Of-Publication\"]) else row[\"Year-Of-Publication\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"✅ Missing Year-Of-Publication values successfully imputed using saved mapping!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['User-ID', 'ISBN', 'Book-Rating', 'Book-Title', 'Book-Author',\n",
       "       'Year-Of-Publication', 'Publisher', 'Image-URL-S', 'Location', 'Age',\n",
       "       'ISBN_fix'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = df['User-ID'].unique()\n",
    "items = df['ISBN_fix'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpm\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# hyperparameters\u001b[39;00m\n\u001b[0;32m      4\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# dimension of latent factors\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymc'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterBayesianVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
