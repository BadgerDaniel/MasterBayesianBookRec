{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1741827886170,"user":{"displayName":"Daniel Sa","userId":"13063706089144668793"},"user_tz":300},"id":"b-Wv5y6J-tUK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18897,"status":"ok","timestamp":1741827905068,"user":{"displayName":"Daniel Sa","userId":"13063706089144668793"},"user_tz":300},"id":"zwL-A3ao-uc1","outputId":"564700ff-9d3e-4f5a-f320-ac2a5ddf277b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18880,"status":"ok","timestamp":1741827923947,"user":{"displayName":"Daniel Sa","userId":"13063706089144668793"},"user_tz":300},"id":"pmALFuOIlVRO","outputId":"3e9b3627-5cb1-4a21-882e-0700c1310f90"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import pymc as pm\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, precision_score, recall_score\n","!pip install openpyxl\n","# Load dataset\n","#df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/book_ratings_cleaned.xlsx\")\n","df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/book_ratings_cleaned.csv\")"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1741827923961,"user":{"displayName":"Daniel Sa","userId":"13063706089144668793"},"user_tz":300},"id":"RSlGMdzbI4EI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1741827923978,"user":{"displayName":"Daniel Sa","userId":"13063706089144668793"},"user_tz":300},"id":"rHVtN6uGJGcm"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"executionInfo":{"elapsed":73,"status":"ok","timestamp":1741827924061,"user":{"displayName":"Daniel Sa","userId":"13063706089144668793"},"user_tz":300},"id":"cOt6fTIFKai2","outputId":"26bba56f-85f4-4f14-ef55-8b34d9b9a82e"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nimport torch\\nprint(\"Is GPU available?\", torch.cuda.is_available())\\n!pip install pymc[jax] --quiet\\n!pip install jax jaxlib --quiet\\n\\n\\ndef evaluate_predictions(true_ratings, predicted_ratings, threshold=7):\\n    mae = mean_absolute_error(true_ratings, predicted_ratings)\\n    rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\\n\\n    # Convert to binary relevance (1 if rating \u003e= threshold, else 0)\\n    true_binary = (true_ratings \u003e= threshold).astype(int)\\n    predicted_binary = (predicted_ratings \u003e= threshold).astype(int)\\n\\n    precision = precision_score(true_binary, predicted_binary, average=\\'micro\\')\\n    recall = recall_score(true_binary, predicted_binary, average=\\'micro\\')\\n\\n    print(f\"MAE: {mae:.4f}\")\\n    print(f\"RMSE: {rmse:.4f}\")\\n    print(f\"Precision: {precision:.4f}\")\\n    print(f\"Recall: {recall:.4f}\")\\n\\n\\n\\nimport arviz as az\\nimport datetime\\nfrom skopt import gp_minimize\\nfrom skopt.space import Real\\n\\n\\nsearch_space = [\\n    Real(0.1, 3.0, name=\"sigma_u\"),\\n    Real(0.1, 3.0, name=\"sigma_b\"),\\n    Real(0.1, 5.0, name=\"alpha_mu\"),\\n    Real(0.1, 5.0, name=\"beta_mu\")\\n]\\n\\n\\ndef objective(params):\\n    sigma_u_val, sigma_b_val, alpha_mu_val, beta_mu_val = params\\n\\n    with pm.Model() as model:\\n        pm.set_data_backend(\"jax\")  # Enables GPU acceleration\\n        pm.set_compute_backend(\"jax\")\\n\\n\\n        mu = pm.Gamma(\"mu\", alpha=alpha_mu_val, beta=beta_mu_val)\\n\\n\\n        user_bias = pm.Normal(\"user_bias\", mu=0, sigma=0.5 / np.sqrt(user_rating_counts + 1), shape=num_users)\\n        book_bias = pm.Normal(\"book_bias\", mu=0, sigma=0.5 / np.sqrt(book_rating_counts + 1), shape=num_books)\\n\\n\\n        sigma_u = pm.HalfCauchy(\"sigma_u\", beta=sigma_u_val)\\n        sigma_b = pm.HalfCauchy(\"sigma_b\", beta=sigma_b_val)\\n\\n        user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\\n        book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\\n\\n\\n        lambda_rating = pm.math.exp(\\n            mu +\\n            user_bias[train_user_ids_small] +\\n            book_bias[train_book_ids_small] +\\n            (user_factors[train_user_ids_small] * book_factors[train_book_ids_small]).sum(axis=1)\\n        )\\n\\n\\n        ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings_small)\\n\\n        approx = pm.fit(n=2500, method=\"advi\")\\n        trace = approx.sample(draws=1000)\\n\\n\\n    predicted_ratings = np.exp(\\n        trace.posterior[\"mu\"].mean().item() +\\n        trace.posterior[\"user_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids_small] +\\n        trace.posterior[\"book_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids_small] +\\n        (trace.posterior[\"user_factors\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids_small] *\\n         trace.posterior[\"book_factors\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids_small]).sum(axis=1)\\n    )\\n\\n\\n    precision = precision_score((test_ratings_small \u003e= 7).astype(int), (predicted_ratings \u003e= 7).astype(int), average=\\'micro\\')\\n\\n    return -precision  # Minimize negative precision (maximize precision)\\n\\n#bayesian Optimization\\nresult = gp_minimize(objective, search_space, n_calls=15, random_state=42, n_jobs=2)\\n\\n# best vals\\nbest_sigma_u, best_sigma_b, best_alpha, best_beta = result.x\\nprint(f\"Optimal sigma_u: {best_sigma_u}, sigma_b: {best_sigma_b}, alpha_mu: {best_alpha}, beta_mu: {best_beta}\")\\n\\nwith pm.Model() as best_model:\\n    mu = pm.Gamma(\"mu\", alpha=best_alpha, beta=best_beta)\\n    user_bias = pm.Normal(\"user_bias\", mu=0, sigma=0.5 / np.sqrt(user_rating_counts + 1), shape=num_users)\\n    book_bias = pm.Normal(\"book_bias\", mu=0, sigma=0.5 / np.sqrt(book_rating_counts + 1), shape=num_books)\\n\\n    sigma_u = pm.HalfCauchy(\"sigma_u\", beta=best_sigma_u)\\n    sigma_b = pm.HalfCauchy(\"sigma_b\", beta=best_sigma_b)\\n\\n    user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\\n    book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\\n\\n    lambda_rating = pm.math.exp(\\n        mu +\\n        user_bias[train_user_ids_small] +\\n        book_bias[train_book_ids_small] +\\n        (user_factors[train_user_ids_small] * book_factors[train_book_ids_small]).sum(axis=1)\\n    )\\n\\n    ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings_small)\\n\\n\\n    approx = pm.fit(n=5000, method=\"advi\")\\n    best_trace = approx.sample(draws=1000)\\n\\ntimestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\\nmodel_filename = f\"/content/drive/MyDrive/Colab Notebooks/best_pymc_model_{timestamp}.nc\"\\naz.to_netcdf(best_trace, model_filename)\\n\\nprint(f\"Saved best PyMC model to {model_filename}\")\\n\\n'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","import torch\n","print(\"Is GPU available?\", torch.cuda.is_available())\n","!pip install pymc[jax] --quiet\n","!pip install jax jaxlib --quiet\n","\n","\n","def evaluate_predictions(true_ratings, predicted_ratings, threshold=7):\n","    mae = mean_absolute_error(true_ratings, predicted_ratings)\n","    rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\n","\n","    # Convert to binary relevance (1 if rating \u003e= threshold, else 0)\n","    true_binary = (true_ratings \u003e= threshold).astype(int)\n","    predicted_binary = (predicted_ratings \u003e= threshold).astype(int)\n","\n","    precision = precision_score(true_binary, predicted_binary, average='micro')\n","    recall = recall_score(true_binary, predicted_binary, average='micro')\n","\n","    print(f\"MAE: {mae:.4f}\")\n","    print(f\"RMSE: {rmse:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","\n","\n","\n","import arviz as az\n","import datetime\n","from skopt import gp_minimize\n","from skopt.space import Real\n","\n","\n","search_space = [\n","    Real(0.1, 3.0, name=\"sigma_u\"),\n","    Real(0.1, 3.0, name=\"sigma_b\"),\n","    Real(0.1, 5.0, name=\"alpha_mu\"),\n","    Real(0.1, 5.0, name=\"beta_mu\")\n","]\n","\n","\n","def objective(params):\n","    sigma_u_val, sigma_b_val, alpha_mu_val, beta_mu_val = params\n","\n","    with pm.Model() as model:\n","        pm.set_data_backend(\"jax\")  # Enables GPU acceleration\n","        pm.set_compute_backend(\"jax\")\n","\n","\n","        mu = pm.Gamma(\"mu\", alpha=alpha_mu_val, beta=beta_mu_val)\n","\n","\n","        user_bias = pm.Normal(\"user_bias\", mu=0, sigma=0.5 / np.sqrt(user_rating_counts + 1), shape=num_users)\n","        book_bias = pm.Normal(\"book_bias\", mu=0, sigma=0.5 / np.sqrt(book_rating_counts + 1), shape=num_books)\n","\n","\n","        sigma_u = pm.HalfCauchy(\"sigma_u\", beta=sigma_u_val)\n","        sigma_b = pm.HalfCauchy(\"sigma_b\", beta=sigma_b_val)\n","\n","        user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\n","        book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\n","\n","\n","        lambda_rating = pm.math.exp(\n","            mu +\n","            user_bias[train_user_ids_small] +\n","            book_bias[train_book_ids_small] +\n","            (user_factors[train_user_ids_small] * book_factors[train_book_ids_small]).sum(axis=1)\n","        )\n","\n","\n","        ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings_small)\n","\n","        approx = pm.fit(n=2500, method=\"advi\")\n","        trace = approx.sample(draws=1000)\n","\n","\n","    predicted_ratings = np.exp(\n","        trace.posterior[\"mu\"].mean().item() +\n","        trace.posterior[\"user_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids_small] +\n","        trace.posterior[\"book_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids_small] +\n","        (trace.posterior[\"user_factors\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids_small] *\n","         trace.posterior[\"book_factors\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids_small]).sum(axis=1)\n","    )\n","\n","\n","    precision = precision_score((test_ratings_small \u003e= 7).astype(int), (predicted_ratings \u003e= 7).astype(int), average='micro')\n","\n","    return -precision  # Minimize negative precision (maximize precision)\n","\n","#bayesian Optimization\n","result = gp_minimize(objective, search_space, n_calls=15, random_state=42, n_jobs=2)\n","\n","# best vals\n","best_sigma_u, best_sigma_b, best_alpha, best_beta = result.x\n","print(f\"Optimal sigma_u: {best_sigma_u}, sigma_b: {best_sigma_b}, alpha_mu: {best_alpha}, beta_mu: {best_beta}\")\n","\n","with pm.Model() as best_model:\n","    mu = pm.Gamma(\"mu\", alpha=best_alpha, beta=best_beta)\n","    user_bias = pm.Normal(\"user_bias\", mu=0, sigma=0.5 / np.sqrt(user_rating_counts + 1), shape=num_users)\n","    book_bias = pm.Normal(\"book_bias\", mu=0, sigma=0.5 / np.sqrt(book_rating_counts + 1), shape=num_books)\n","\n","    sigma_u = pm.HalfCauchy(\"sigma_u\", beta=best_sigma_u)\n","    sigma_b = pm.HalfCauchy(\"sigma_b\", beta=best_sigma_b)\n","\n","    user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\n","    book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\n","\n","    lambda_rating = pm.math.exp(\n","        mu +\n","        user_bias[train_user_ids_small] +\n","        book_bias[train_book_ids_small] +\n","        (user_factors[train_user_ids_small] * book_factors[train_book_ids_small]).sum(axis=1)\n","    )\n","\n","    ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings_small)\n","\n","\n","    approx = pm.fit(n=5000, method=\"advi\")\n","    best_trace = approx.sample(draws=1000)\n","\n","timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","model_filename = f\"/content/drive/MyDrive/Colab Notebooks/best_pymc_model_{timestamp}.nc\"\n","az.to_netcdf(best_trace, model_filename)\n","\n","print(f\"Saved best PyMC model to {model_filename}\")\n","\n","'''"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33567,"status":"ok","timestamp":1741827957626,"user":{"displayName":"Daniel Sa","userId":"13063706089144668793"},"user_tz":300},"id":"sivGzWqaFDSM","outputId":"0e5008d2-958e-4d44-fc9a-ef74911cbab5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/501.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m501.8/501.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m501.9/501.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/360.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m358.4/360.8 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.8/360.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hIs GPU available? False\n"]},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-4-0736057f3167\u003e:30: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['User-Index'] = df['User-ID'].astype(\"category\").cat.codes\n"]},{"name":"stdout","output_type":"stream","text":["Number of unique users: 28868, Number of unique books: 141472\n","Using 2756 training rows and 1378 test rows.\n"]}],"source":["\n","!pip install pymc==5.19.1 numpyro jax jaxlib scikit-optimize --quiet\n","\n","import numpy as np\n","import pandas as pd\n","import pymc as pm\n","import arviz as az\n","import datetime\n","import torch\n","from skopt import gp_minimize\n","from skopt.space import Real\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","\n","\n","print(\"Is GPU available?\", torch.cuda.is_available())\n","\n","\n","#df = pd.read_excel(\"book_ratings.xlsx\")\n","\n","\n","df = df[['User-ID', 'ISBN', 'Book-Rating']]\n","df = df[df['Book-Rating'] \u003e 0]  # Remove zero ratings\n","\n","# filter out users with too few reviews\n","user_counts = df['User-ID'].value_counts()\n","valid_users = user_counts[user_counts \u003e= 2].index\n","df = df[df['User-ID'].isin(valid_users)]\n","\n","# encode user-ID and ISBN as categorical indices\n","df['User-Index'] = df['User-ID'].astype(\"category\").cat.codes\n","df['Book-Index'] = df['ISBN'].astype(\"category\").cat.codes\n","\n","# make sure we have a contiguous range for indexing\n","df['User-Index'] = df['User-Index'].astype(\"category\").cat.codes\n","df['Book-Index'] = df['Book-Index'].astype(\"category\").cat.codes\n","\n","# rating counts per user and book\n","user_rating_counts = df.groupby('User-Index')['Book-Rating'].count().values\n","book_rating_counts = df.groupby('Book-Index')['Book-Rating'].count().values\n","\n","### revent division by zero\n","user_rating_counts[user_rating_counts == 0] = 1\n","book_rating_counts[book_rating_counts == 0] = 1\n","\n","\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# convert to np arrays for faster access\n","train_user_ids = train_df['User-Index'].values\n","test_user_ids = test_df['User-Index'].values\n","train_book_ids = train_df['Book-Index'].values\n","test_book_ids = test_df['Book-Index'].values\n","train_ratings = train_df['Book-Rating'].values\n","test_ratings = test_df['Book-Rating'].values\n","\n","# **Update counts after filtering**\n","num_users = df['User-Index'].nunique()\n","num_books = df['Book-Index'].nunique()\n","print(f\"Number of unique users: {num_users}, Number of unique books: {num_books}\")\n","\n","# reducing datasize (hardware limitation)\n","use_full_dataset = False\n","latent_dim = 3 ### hardware limitation\n","\n","if use_full_dataset:\n","    train_user_ids_small, train_book_ids_small, train_ratings_small = train_user_ids, train_book_ids, train_ratings\n","    test_user_ids_small, test_book_ids_small, test_ratings_small = test_user_ids, test_book_ids, test_ratings\n","else:\n","    subset_fraction = 0.01  # 1% of training data\n","    subset_size = int(len(train_df) * subset_fraction)\n","    subset_size = max(1000, min(subset_size, 20000))\n","\n","    train_idx_subset = np.random.choice(len(train_user_ids), subset_size, replace=False)\n","    test_idx_subset = np.random.choice(len(test_user_ids), min(subset_size // 2, len(test_user_ids)), replace=False)\n","\n","    train_user_ids_small, train_book_ids_small, train_ratings_small = (\n","        train_user_ids[train_idx_subset],\n","        train_book_ids[train_idx_subset],\n","        train_ratings[train_idx_subset],\n","    )\n","\n","    test_user_ids_small, test_book_ids_small, test_ratings_small = (\n","        test_user_ids[test_idx_subset],\n","        test_book_ids[test_idx_subset],\n","        test_ratings[test_idx_subset],\n","    )\n","\n","    print(f\"Using {subset_size} training rows and {len(test_user_ids_small)} test rows.\")\n","\n","#### evaluate our preds\n","def evaluate_predictions(true_ratings, predicted_ratings, threshold=7):\n","    mae = mean_absolute_error(true_ratings, predicted_ratings)\n","    rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\n","    true_binary = (true_ratings \u003e= threshold).astype(int)\n","    predicted_binary = (predicted_ratings \u003e= threshold).astype(int)\n","    precision = precision_score(true_binary, predicted_binary, average='micro')\n","    recall = recall_score(true_binary, predicted_binary, average='micro')\n","    print(f\"MAE: {mae:.10f}, RMSE: {rmse:.10f}, Precision: {precision:.10f}, Recall: {recall:.10f}\")\n","\n","# limited bayesian optimization search space --\u003e limited by hardware\n","search_space = [\n","    Real(0.1, 3.0, name=\"sigma_u\"),\n","    Real(0.1, 3.0, name=\"sigma_b\"),\n","    Real(0.1, 5.0, name=\"alpha_mu\"),\n","    Real(0.1, 5.0, name=\"beta_mu\"),\n","]\n","\n","# objective function for optimization\n","def objective(params):\n","    sigma_u_val, sigma_b_val, alpha_mu_val, beta_mu_val = params\n","    with pm.Model() as model:\n","        mu = pm.Gamma(\"mu\", alpha=alpha_mu_val, beta=beta_mu_val)\n","        user_bias = pm.Normal(\"user_bias\", mu=0, sigma=0.5 / np.sqrt(user_rating_counts + 1), shape=num_users)\n","        book_bias = pm.Normal(\"book_bias\", mu=0, sigma=0.5 / np.sqrt(book_rating_counts + 1), shape=num_books)\n","        sigma_u = pm.HalfCauchy(\"sigma_u\", beta=sigma_u_val)\n","        sigma_b = pm.HalfCauchy(\"sigma_b\", beta=sigma_b_val)\n","        user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\n","        book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\n","        lambda_rating = pm.math.exp(mu + user_bias[train_user_ids_small] + book_bias[train_book_ids_small] +\n","                                    (user_factors[train_user_ids_small] * book_factors[train_book_ids_small]).sum(axis=1))\n","        ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings_small)\n","        approx = pm.fit(n=2500, method=\"advi\")\n","        trace = approx.sample(draws=500)  # reduced draws to minimize RAM usage --- hardware limitation\n","\n","    predicted_ratings = np.exp(trace.posterior[\"mu\"].mean().item() +\n","                               trace.posterior[\"user_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids_small] +\n","                               trace.posterior[\"book_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids_small] +\n","                               (trace.posterior[\"user_factors\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids_small] *\n","                                trace.posterior[\"book_factors\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids_small]).sum(axis=1))\n","\n","    precision = precision_score((test_ratings_small \u003e= 7).astype(int), (predicted_ratings \u003e= 7).astype(int), average='micro')\n","    return -precision\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":57,"status":"ok","timestamp":1741827957689,"user":{"displayName":"Daniel Sa","userId":"13063706089144668793"},"user_tz":300},"id":"eOI1xaNF1e1V"},"outputs":[],"source":["# **Run Bayesian Optimization**\n","#result = gp_minimize(objective, search_space, n_calls=10, random_state=42, n_jobs=1)\n","\n","#print(f\"Optimal parameters: {result.x}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1741827957694,"user":{"displayName":"Daniel Sa","userId":"13063706089144668793"},"user_tz":300},"id":"YyfmKvn90-F5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"C8Szt0Z0RDyh"},"outputs":[{"name":"stderr","output_type":"stream","text":["warmup:   4%|▍         | 45/1000 [1:39:15\u003c54:12:16, 204.33s/it]"]}],"source":["# retraining final model\n","\n","best_sigma_u = 1.2595968179742412\n","best_sigma_b = 0.23533042331948476\n","best_alpha = 4.871402042323151\n","best_beta = 1.2405795681084912\n","\n","with pm.Model() as best_model:\n","    mu = pm.Gamma(\"mu\", alpha=best_alpha, beta=best_beta)\n","    user_bias = pm.Normal(\"user_bias\", mu=0, sigma=0.5 / np.sqrt(user_rating_counts + 1), shape=num_users)\n","    book_bias = pm.Normal(\"book_bias\", mu=0, sigma=0.5 / np.sqrt(book_rating_counts + 1), shape=num_books)\n","\n","    sigma_u = pm.HalfCauchy(\"sigma_u\", beta=best_sigma_u)\n","    sigma_b = pm.HalfCauchy(\"sigma_b\", beta=best_sigma_b)\n","\n","    user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\n","    book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\n","\n","    lambda_rating = pm.math.exp(\n","        mu + user_bias[train_user_ids_small] + book_bias[train_book_ids_small] +\n","        (user_factors[train_user_ids_small] * book_factors[train_book_ids_small]).sum(axis=1)\n","    )\n","\n","    ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings_small)\n","\n","    # using jax to accelarate the sampling (numpyro) and utilize multiprocessing\n","    best_trace = pm.sample(\n","        draws=500, tune=500, chains=2,\n","        nuts_sampler=\"numpyro\",\n","        nuts_sampler_kwargs={\"chain_method\": \"vectorized\"}\n","    )\n","\n","\n","timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","model_filename = f\"best_pymc_model_{timestamp}.nc\"\n","az.to_netcdf(best_trace, model_filename)\n","\n","print(f\"Saved best PyMC model to {model_filename}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YTZHYhVFRp3Y"},"outputs":[],"source":["model_filename = f\"/content/drive/MyDrive/Colab Notebooks/best_pymc_model_{timestamp}.nc\"\n","az.to_netcdf(best_trace, model_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIzRD97rKc0v"},"outputs":[],"source":["\n","import datetime\n","timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","filename = f\"/content/drive/MyDrive/beta_alpha_sigma_{timestamp}.txt\"\n","with open(filename, \"w\") as f:\n","    f.write(f\"Optimal sigma_u: {best_sigma_u}\\n\")\n","    f.write(f\"Optimal sigma_b: {best_sigma_b}\\n\")\n","    f.write(f\"Optimal alpha_mu: {best_alpha}\\n\")\n","    f.write(f\"Optimal beta_mu: {best_beta}\\n\")\n","\n","print(f\"Saved parameters to {filename}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djtb5WQ-KiJm"},"outputs":[],"source":["\n","\n","best_sigma_u, best_sigma_b, best_alpha, best_beta = result.x\n","print(f\"Optimal sigma_u: {best_sigma_u}, sigma_b: {best_sigma_b}, alpha_mu: {best_alpha}, beta_mu: {best_beta}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CvDheP1L4yMr"},"outputs":[],"source":["import numpy as np\n","import arviz as az\n","from sklearn.metrics import precision_score\n","\n","# Extract posterior mean values from the trace\n","mu_post = best_trace.posterior[\"mu\"].mean().item()\n","user_bias_post = best_trace.posterior[\"user_bias\"].mean(dim=(\"chain\", \"draw\")).values\n","book_bias_post = best_trace.posterior[\"book_bias\"].mean(dim=(\"chain\", \"draw\")).values\n","user_factors_post = best_trace.posterior[\"user_factors\"].mean(dim=(\"chain\", \"draw\")).values\n","book_factors_post = best_trace.posterior[\"book_factors\"].mean(dim=(\"chain\", \"draw\")).values\n","\n","# Compute predicted ratings\n","predicted_ratings = np.exp(\n","    mu_post +\n","    user_bias_post[test_user_ids_small] +\n","    book_bias_post[test_book_ids_small] +\n","    (user_factors_post[test_user_ids_small] * book_factors_post[test_book_ids_small]).sum(axis=1)\n",")\n","\n","# Convert to binary format for precision calculation\n","threshold = 7\n","true_binary = (test_ratings_small \u003e= threshold).astype(int)\n","predicted_binary = (predicted_ratings \u003e= threshold).astype(int)\n","\n","# Compute precision\n","precision = precision_score(true_binary, predicted_binary, average='micro')\n","\n","print(f\"Precision: {precision:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"22eh_Fj_Kh3D"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","provenance":[{"file_id":"1Y_Euf43__WoZ5PSsHLMW4t9vu8cfmdrq","timestamp":1741391852569}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}